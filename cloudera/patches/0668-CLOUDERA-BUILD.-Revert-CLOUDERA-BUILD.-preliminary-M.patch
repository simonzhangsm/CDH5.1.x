From e8895df1932568faeb10186eca34f1f1505c277f Mon Sep 17 00:00:00 2001
From: Robert Kanter <rkanter@cloudera.com>
Date: Mon, 2 Jun 2014 13:55:39 -0700
Subject: [PATCH 668/795] CLOUDERA-BUILD. Revert "CLOUDERA-BUILD. preliminary MAPREDUCE-5875. Make Counter limits consistent conf across JobClient, MRAppMaster, and YarnChild (CDH-18794)"

This reverts commit d533d6a198926f6c1a83dea8712ed3f519e5852b.
---
 .../hadoop/mapreduce/v2/app/MRAppMaster.java       |    4 +-
 .../java/org/apache/hadoop/mapreduce/Cluster.java  |   16 ++--
 .../org/apache/hadoop/mapreduce/JobSubmitter.java  |    2 -
 .../apache/hadoop/mapreduce/counters/Limits.java   |    5 -
 .../hadoop/mapreduce/jobhistory/HistoryViewer.java |   18 +----
 .../hadoop/mapreduce/v2/hs/CompletedJob.java       |   15 ----
 .../org/apache/hadoop/mapreduce/v2/TestMRJobs.java |   80 ++------------------
 7 files changed, 18 insertions(+), 122 deletions(-)

diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java
index 35e16e0..e39fa1d 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java
@@ -41,6 +41,7 @@
 import org.apache.hadoop.fs.FSDataInputStream;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.http.HttpConfig;
 import org.apache.hadoop.mapred.FileOutputCommitter;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.LocalContainerLauncher;
@@ -53,7 +54,6 @@
 import org.apache.hadoop.mapreduce.TaskAttemptContext;
 import org.apache.hadoop.mapreduce.TaskAttemptID;
 import org.apache.hadoop.mapreduce.TypeConverter;
-import org.apache.hadoop.mapreduce.counters.Limits;
 import org.apache.hadoop.mapreduce.jobhistory.AMStartedEvent;
 import org.apache.hadoop.mapreduce.jobhistory.EventReader;
 import org.apache.hadoop.mapreduce.jobhistory.EventType;
@@ -1076,8 +1076,6 @@ protected void serviceStart() throws Exception {
 
     // set job classloader if configured
     MRApps.setJobClassLoader(getConfig());
-    // Initing with our JobConf allows us to avoid loading confs twice
-    Limits.init(getConfig());
     // All components have started, start the job.
     startJobs();
   }
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java
index 60ff715..2fcc046 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java
@@ -182,15 +182,15 @@ public FileSystem run() throws IOException, InterruptedException {
   public Job getJob(JobID jobId) throws IOException, InterruptedException {
     JobStatus status = client.getJobStatus(jobId);
     if (status != null) {
-      final JobConf conf = new JobConf();
-      final Path jobPath = new Path(client.getFilesystemName(),
-          status.getJobFile());
-      final FileSystem fs = FileSystem.get(jobPath.toUri(), getConf());
+      JobConf conf;
       try {
-        conf.addResource(fs.open(jobPath), jobPath.toString());
-      } catch (FileNotFoundException fnf) {
-        if (LOG.isWarnEnabled()) {
-          LOG.warn("Job conf missing on cluster", fnf);
+        conf = new JobConf(status.getJobFile());
+      } catch (RuntimeException ex) {
+        // If job file doesn't exist it means we can't find the job
+        if (ex.getCause() instanceof FileNotFoundException) {
+          return null;
+        } else {
+          throw ex;
         }
       }
       return Job.getInstance(this, status, conf);
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java
index f4d93a5..94e7125 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java
@@ -49,7 +49,6 @@
 import org.apache.hadoop.mapred.QueueACL;
 import static org.apache.hadoop.mapred.QueueManager.toFullPropertyName;
 
-import org.apache.hadoop.mapreduce.counters.Limits;
 import org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager;
 import org.apache.hadoop.mapreduce.filecache.DistributedCache;
 import org.apache.hadoop.mapreduce.protocol.ClientProtocol;
@@ -425,7 +424,6 @@ JobStatus submitJobInternal(Job job, Cluster cluster)
 
       // Write job file to submit dir
       writeConf(conf, submitJobFile);
-      Limits.reset(conf);
       
       //
       // Now, actually submit the job (using the submit name)
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/counters/Limits.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/counters/Limits.java
index cdce243..aa16967 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/counters/Limits.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/counters/Limits.java
@@ -125,9 +125,4 @@ public synchronized void checkGroups(int size) {
   public synchronized LimitExceededException violation() {
     return firstViolation;
   }
-
-  public static synchronized void reset(Configuration conf) {
-    isInited = false;
-    init(conf);
-  }
 }
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/HistoryViewer.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/HistoryViewer.java
index 43b2df2..eaeadea 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/HistoryViewer.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/jobhistory/HistoryViewer.java
@@ -17,7 +17,6 @@
  */
 package org.apache.hadoop.mapreduce.jobhistory;
 
-import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.text.DecimalFormat;
 import java.text.Format;
@@ -30,8 +29,6 @@
 import java.util.Set;
 import java.util.TreeSet;
 
-import org.apache.commons.logging.Log;
-import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.classification.InterfaceAudience;
 import org.apache.hadoop.classification.InterfaceStability;
 import org.apache.hadoop.conf.Configuration;
@@ -44,7 +41,6 @@
 import org.apache.hadoop.mapreduce.TaskAttemptID;
 import org.apache.hadoop.mapreduce.TaskID;
 import org.apache.hadoop.mapreduce.TaskType;
-import org.apache.hadoop.mapreduce.counters.Limits;
 import org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.JobInfo;
 import org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.TaskInfo;
 import org.apache.hadoop.mapreduce.util.HostUtil;
@@ -58,8 +54,7 @@
 @InterfaceAudience.Private
 @InterfaceStability.Unstable
 public class HistoryViewer {
-  private static final Log LOG = LogFactory.getLog(HistoryViewer.class);
-  private static final SimpleDateFormat dateFormat =
+  private static SimpleDateFormat dateFormat = 
     new SimpleDateFormat("d-MMM-yyyy HH:mm:ss");
   private FileSystem fs;
   private JobInfo job;
@@ -88,17 +83,6 @@ public HistoryViewer(String historyFile,
         System.err.println("Ignore unrecognized file: " + jobFile.getName());
         throw new IOException(errorMsg);
       }
-      final Path jobConfPath = new Path(jobFile.getParent(),  jobDetails[0]
-          + "_" + jobDetails[1] + "_" + jobDetails[2] + "_conf.xml");
-      final Configuration jobConf = new Configuration(conf);
-      try {
-        jobConf.addResource(fs.open(jobConfPath), jobConfPath.toString());
-        Limits.reset(conf);
-      } catch (FileNotFoundException fnf) {
-        if (LOG.isWarnEnabled()) {
-          LOG.warn("Missing job conf in history", fnf);
-        }
-      }
       JobHistoryParser parser = new JobHistoryParser(fs, jobFile);
       job = parser.parse();
       jobId = job.getJobId().toString();
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CompletedJob.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CompletedJob.java
index 7330378..049a389 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CompletedJob.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CompletedJob.java
@@ -18,7 +18,6 @@
 
 package org.apache.hadoop.mapreduce.v2.hs;
 
-import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.net.UnknownHostException;
 import java.util.ArrayList;
@@ -35,7 +34,6 @@
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.mapred.JobACLsManager;
 import org.apache.hadoop.mapred.TaskCompletionEvent;
@@ -43,7 +41,6 @@
 import org.apache.hadoop.mapreduce.JobACL;
 import org.apache.hadoop.mapreduce.TaskID;
 import org.apache.hadoop.mapreduce.TypeConverter;
-import org.apache.hadoop.mapreduce.counters.Limits;
 import org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser;
 import org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.JobInfo;
 import org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.TaskInfo;
@@ -335,21 +332,9 @@ protected synchronized void loadFullHistoryData(boolean loadTasks,
       verifyHistoryExistsAndNotEmpty(historyFileAbsolute);
       JobHistoryParser parser = null;
       try {
-        final FileSystem fs = historyFileAbsolute.getFileSystem(conf);
         parser =
             new JobHistoryParser(historyFileAbsolute.getFileSystem(conf),
                 historyFileAbsolute);
-        final Path jobConfPath = new Path(historyFileAbsolute.getParent(),
-            JobHistoryUtils.getIntermediateConfFileName(jobId));
-        final Configuration conf = new Configuration();
-        try {
-          conf.addResource(fs.open(jobConfPath), jobConfPath.toString());
-          Limits.reset(conf);
-        } catch (FileNotFoundException fnf) {
-          if (LOG.isWarnEnabled()) {
-            LOG.warn("Missing job conf in history", fnf);
-          }
-        }
         this.jobInfo = parser.parse();
       } catch (IOException e) {
         throw new YarnRuntimeException("Could not load history file "
diff --git a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestMRJobs.java b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestMRJobs.java
index def6a60..2027d37 100644
--- a/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestMRJobs.java
+++ b/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestMRJobs.java
@@ -53,14 +53,10 @@
 import org.apache.hadoop.fs.permission.FsPermission;
 import org.apache.hadoop.hdfs.MiniDFSCluster;
 import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.JobClient;
 import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.JobID;
-import org.apache.hadoop.mapred.RunningJob;
 import org.apache.hadoop.mapred.TaskLog;
 import org.apache.hadoop.mapreduce.Counters;
 import org.apache.hadoop.mapreduce.Job;
@@ -104,7 +100,6 @@
       EnumSet.of(RMAppState.FINISHED, RMAppState.FAILED, RMAppState.KILLED);
   private static final int NUM_NODE_MGRS = 3;
   private static final String TEST_IO_SORT_MB = "11";
-  private static final String TEST_GROUP_MAX = "200";
 
   protected static MiniMRYarnCluster mrCluster;
   protected static MiniDFSCluster dfsCluster;
@@ -213,83 +208,33 @@ public void testSleepJob() throws IOException, InterruptedException,
   }
 
   @Test(timeout = 300000)
-  public void testConfVerificationWithClassloader() throws Exception {
-    testConfVerification(true, false, false);
-  }
-
-  @Test(timeout = 300000)
-  public void testConfVerificationWithOutClassloader() throws Exception {
-    testConfVerification(false, false, false);
-  }
-
-  @Test(timeout = 300000)
-  public void testConfVerificationWithJobClient() throws Exception {
-    testConfVerification(false, true, false);
-  }
-
-  @Test//(timeout = 300000)
-  public void testConfVerificationWithJobClientLocal() throws Exception {
-    testConfVerification(false, true, true);
-  }
-
-  private void testConfVerification(boolean useJobClassLoader,
-      boolean useJobClientForMonitring, boolean useLocal) throws Exception {
-    LOG.info("\n\n\nStarting testConfVerification()"
-        + " jobClassloader=" + useJobClassLoader
-        + " jobClient=" + useJobClientForMonitring
-        + " localMode=" + useLocal);
+  public void testJobClassloader() throws IOException, InterruptedException,
+      ClassNotFoundException {
+    LOG.info("\n\n\nStarting testJobClassloader().");
 
     if (!(new File(MiniMRYarnCluster.APPJAR)).exists()) {
       LOG.info("MRAppJar " + MiniMRYarnCluster.APPJAR
                + " not found. Not running test.");
       return;
     }
-    final Configuration clusterConfig;
-    if (useLocal) {
-      clusterConfig = new Configuration();
-      conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);
-    } else {
-      clusterConfig = mrCluster.getConfig();
-    }
-    final JobClient jc = new JobClient(clusterConfig);
-    final Configuration sleepConf = new Configuration(clusterConfig);
+    final Configuration sleepConf = new Configuration(mrCluster.getConfig());
     // set master address to local to test that local mode applied iff framework == local
     sleepConf.set(MRConfig.MASTER_ADDRESS, "local");
-    sleepConf.setBoolean(MRJobConfig.MAPREDUCE_JOB_CLASSLOADER,
-        useJobClassLoader);
+    sleepConf.setBoolean(MRJobConfig.MAPREDUCE_JOB_CLASSLOADER, true);
     sleepConf.set(MRJobConfig.IO_SORT_MB, TEST_IO_SORT_MB);
     sleepConf.set(MRJobConfig.MR_AM_LOG_LEVEL, Level.ALL.toString());
     sleepConf.set(MRJobConfig.MAP_LOG_LEVEL, Level.ALL.toString());
     sleepConf.set(MRJobConfig.REDUCE_LOG_LEVEL, Level.ALL.toString());
-    sleepConf.set(MRJobConfig.COUNTER_GROUPS_MAX_KEY, TEST_GROUP_MAX);
+    sleepConf.set(MRJobConfig.MAP_JAVA_OPTS, "-verbose:class");
     final SleepJob sleepJob = new SleepJob();
     sleepJob.setConf(sleepConf);
-    final Job job = sleepJob.createJob(1, 1, 1000, 1, 10, 1);
+    final Job job = sleepJob.createJob(1, 1, 10, 1, 10, 1);
     job.setMapperClass(ConfVerificationMapper.class);
     job.addFileToClassPath(APP_JAR); // The AppMaster jar itself.
     job.setJarByClass(SleepJob.class);
     job.setMaxMapAttempts(1); // speed up failures
     job.submit();
-    final boolean succeeded;
-    if (useJobClientForMonitring && !useLocal) {
-      // We can't use getJobID in useLocal case because JobClient and Job
-      // point to different instances of LocalJobRunner
-      //
-      final JobID mapredJobID = JobID.downgrade(job.getJobID());
-      RunningJob runningJob = null;
-      do {
-        Thread.sleep(10);
-        runningJob = jc.getJob(mapredJobID);
-      } while (runningJob == null);
-      Assert.assertEquals("Unexpected RunningJob's "
-          + MRJobConfig.COUNTER_GROUPS_MAX_KEY,
-          TEST_GROUP_MAX, runningJob.getConfiguration()
-              .get(MRJobConfig.COUNTER_GROUPS_MAX_KEY));
-      runningJob.waitForCompletion();
-      succeeded = runningJob.isSuccessful();
-    } else {
-      succeeded = job.waitForCompletion(true);
-    }
+    boolean succeeded = job.waitForCompletion(true);
     Assert.assertTrue("Job status: " + job.getStatus().getFailureInfo(),
         succeeded);
   }
@@ -897,14 +842,5 @@ protected void setup(Context context)
             + ", actual: "  + ioSortMb);
       }
     }
-
-    @Override
-    public void map(IntWritable key, IntWritable value, Context context) throws IOException, InterruptedException {
-      super.map(key, value, context);
-      for (int i = 0; i < 100; i++) {
-        context.getCounter("testCounterGroup-" + i,
-            "testCounter").increment(1);
-      }
-    }
   }
 }
-- 
1.7.0.4

